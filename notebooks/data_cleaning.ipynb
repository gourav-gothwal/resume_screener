{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d7cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e8dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.read_csv(\"D:/resume screener/datasets/resume_dataset.csv\")\n",
    "jobs = pd.read_csv(\"D:/resume screener/datasets/job_posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32271ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 2)\n",
      "Index(['Category', 'Resume'], dtype='object')\n",
      "       Category                                             Resume\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n"
     ]
    }
   ],
   "source": [
    "print(resumes.shape)\n",
    "print(resumes.columns)\n",
    "print(resumes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a8e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19001, 24)\n",
      "Index(['jobpost', 'date', 'Title', 'Company', 'AnnouncementCode', 'Term',\n",
      "       'Eligibility', 'Audience', 'StartDate', 'Duration', 'Location',\n",
      "       'JobDescription', 'JobRequirment', 'RequiredQual', 'Salary',\n",
      "       'ApplicationP', 'OpeningDate', 'Deadline', 'Notes', 'AboutC', 'Attach',\n",
      "       'Year', 'Month', 'IT'],\n",
      "      dtype='object')\n",
      "                                             jobpost          date  \\\n",
      "0  AMERIA Investment Consulting Company\\r\\nJOB TI...   Jan 5, 2004   \n",
      "1  International Research & Exchanges Board (IREX...   Jan 7, 2004   \n",
      "2  Caucasus Environmental NGO Network (CENN)\\r\\nJ...   Jan 7, 2004   \n",
      "3  Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...   Jan 7, 2004   \n",
      "4  Yerevan Brandy Company\\r\\nJOB TITLE:  Software...  Jan 10, 2004   \n",
      "\n",
      "                                               Title  \\\n",
      "0                            Chief Financial Officer   \n",
      "1  Full-time Community Connections Intern (paid i...   \n",
      "2                                Country Coordinator   \n",
      "3                                     BCC Specialist   \n",
      "4                                 Software Developer   \n",
      "\n",
      "                                           Company AnnouncementCode Term  \\\n",
      "0             AMERIA Investment Consulting Company              NaN  NaN   \n",
      "1  International Research & Exchanges Board (IREX)              NaN  NaN   \n",
      "2        Caucasus Environmental NGO Network (CENN)              NaN  NaN   \n",
      "3                                     Manoff Group              NaN  NaN   \n",
      "4                           Yerevan Brandy Company              NaN  NaN   \n",
      "\n",
      "  Eligibility Audience StartDate                               Duration  ...  \\\n",
      "0         NaN      NaN       NaN                                    NaN  ...   \n",
      "1         NaN      NaN       NaN                               3 months  ...   \n",
      "2         NaN      NaN       NaN  Renewable annual contract\\r\\nPOSITION  ...   \n",
      "3         NaN      NaN       NaN                                    NaN  ...   \n",
      "4         NaN      NaN       NaN                                    NaN  ...   \n",
      "\n",
      "  Salary                                       ApplicationP OpeningDate  \\\n",
      "0    NaN  To apply for this position, please submit a\\r\\...         NaN   \n",
      "1    NaN  Please submit a cover letter and resume to:\\r\\...         NaN   \n",
      "2    NaN  Please send resume or CV toursula.kazarian@......         NaN   \n",
      "3    NaN  Please send cover letter and resume to Amy\\r\\n...         NaN   \n",
      "4    NaN  Successful candidates should submit\\r\\n- CV; \\...         NaN   \n",
      "\n",
      "                                        Deadline Notes  \\\n",
      "0                                26 January 2004   NaN   \n",
      "1                                12 January 2004   NaN   \n",
      "2  20 January 2004\\r\\nSTART DATE:  February 2004   NaN   \n",
      "3      23 January 2004\\r\\nSTART DATE:  Immediate   NaN   \n",
      "4                         20 January 2004, 18:00   NaN   \n",
      "\n",
      "                                              AboutC Attach  Year Month     IT  \n",
      "0                                                NaN    NaN  2004     1  False  \n",
      "1  The International Research & Exchanges Board (...    NaN  2004     1  False  \n",
      "2  The Caucasus Environmental NGO Network is a\\r\\...    NaN  2004     1  False  \n",
      "3                                                NaN    NaN  2004     1  False  \n",
      "4                                                NaN    NaN  2004     1   True  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(jobs.shape)\n",
    "print(jobs.columns)\n",
    "print(jobs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfd0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', ' ', str(text))\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "resumes['cleaned_text'] = resumes['Resume'].apply(clean_text)\n",
    "jobs['Category'] = resumes['Category'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53ddc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobs[['Title', 'JobDescription', 'RequiredQual']]\n",
    "jobs = jobs.fillna(\"\")\n",
    "for col in ['Title', 'JobDescription', 'RequiredQual']:\n",
    "    jobs[col] = jobs[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab6ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Resume  \\\n",
      "0  skills programming languages python pandas num...   \n",
      "1  skills programming languages python pandas num...   \n",
      "2  education details may 2013 to may 2017 b e uit...   \n",
      "3  education details may 2013 to may 2017 b e uit...   \n",
      "4  areas of interest deep learning control system...   \n",
      "\n",
      "                                            Job_Desc  Label  \n",
      "0  cnfa seeks qualified candidates for the water ...      1  \n",
      "1  the selected candidate will work as a team mem...      0  \n",
      "2  the database administrator will be responsible...      1  \n",
      "3  sunfood llc is seeking a qualified sales direc...      0  \n",
      "4  working with local and global teams the role w...      1  \n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "pairs = []\n",
    "\n",
    "for idx, row in resumes.iterrows():\n",
    "    resume_text = row['cleaned_text']\n",
    "    category = row['Category']\n",
    "\n",
    "    # Positive job match\n",
    "    positive_jobs = jobs[jobs['Title'].str.contains(category.split()[0], case=False, na=False)]\n",
    "    if not positive_jobs.empty:\n",
    "        job_row = positive_jobs.sample(1).iloc[0]\n",
    "        pairs.append([resume_text, job_row['JobDescription'], 1])\n",
    "\n",
    "    # Negative job match\n",
    "    negative_jobs = jobs[~jobs['Title'].str.contains(category.split()[0], case=False, na=False)]\n",
    "    if not negative_jobs.empty:\n",
    "        job_row = negative_jobs.sample(1).iloc[0]\n",
    "        pairs.append([resume_text, job_row['JobDescription'], 0])\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Resume', 'Job_Desc', 'Label'])\n",
    "print(pairs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137a470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df.to_csv(\"D:/resume screener/datasets/training_pairs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3c514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3cf8fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gg185\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gg185\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea806bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec25155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Resume_clean  \\\n",
      "0  skill programming language python panda numpy ...   \n",
      "1  skill programming language python panda numpy ...   \n",
      "2  education detail may 2013 may 2017 b e uit rgp...   \n",
      "3  education detail may 2013 may 2017 b e uit rgp...   \n",
      "4  area interest deep learning control system des...   \n",
      "\n",
      "                                      Job_Desc_clean  Label  \n",
      "0  cnfa seek qualified candidate water market wtm...      1  \n",
      "1  selected candidate work team member company te...      0  \n",
      "2  database administrator responsible managing ma...      1  \n",
      "3  sunfood llc seeking qualified sale director ef...      0  \n",
      "4  working local global team role initially suppo...      1  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    text = re.sub(r'/d+', '', text)\n",
    "    text = text.strip()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "pairs_df['Resume_clean'] = pairs_df['Resume'].apply(clean_text)\n",
    "pairs_df['Job_Desc_clean'] = pairs_df['Job_Desc'].apply(clean_text)\n",
    "\n",
    "print(pairs_df[['Resume_clean', 'Job_Desc_clean', 'Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f708a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb4e088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72af020a68f54c378d32742f22eeceef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gg185\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gg185\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bff07d7ac442f1b5c6e94c6da7a1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bb1ed080ea43eb9b8f37a843bbec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de86eebefc1447c5867948b3f73d7e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33d13d62ed445b48a237028df9d8bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25169fbcb5c84979acc1b72472f616cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca96d829e4d742f5b2dd677a347207c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fdd18c945a46c1802d2c356dbce880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db14a1ea44540babd6f25caf0fa539d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3eb634ed6d4b56aabe55df1d7c74b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dfbc678be4497883e873dcbcc65a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "221539b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings = model_bert.encode(pairs_df['Resume_clean'].tolist(), convert_to_numpy=True)\n",
    "job_embeddings = model_bert.encode(pairs_df['Job_Desc_clean'].tolist(), convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f2003f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([resume_embeddings, job_embeddings])\n",
    "y = pairs_df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b67cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6694214876033058\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71       196\n",
      "           1       0.67      0.56      0.61       167\n",
      "\n",
      "    accuracy                           0.67       363\n",
      "   macro avg       0.67      0.66      0.66       363\n",
      "weighted avg       0.67      0.67      0.67       363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f8a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ea4b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sbert_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"resume_match_model.pkl\")\n",
    "joblib.dump(model_bert, \"sbert_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
