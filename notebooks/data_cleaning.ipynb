{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d7cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e8dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pd.read_csv(\"D:/resume screener/my_datasets/resume_dataset.csv\")\n",
    "jobs = pd.read_csv(\"D:/resume screener/my_datasets/job_posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32271ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 2)\n",
      "Index(['Category', 'Resume'], dtype='object')\n",
      "       Category                                             Resume\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n"
     ]
    }
   ],
   "source": [
    "print(resumes.shape)\n",
    "print(resumes.columns)\n",
    "print(resumes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a8e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19001, 24)\n",
      "Index(['jobpost', 'date', 'Title', 'Company', 'AnnouncementCode', 'Term',\n",
      "       'Eligibility', 'Audience', 'StartDate', 'Duration', 'Location',\n",
      "       'JobDescription', 'JobRequirment', 'RequiredQual', 'Salary',\n",
      "       'ApplicationP', 'OpeningDate', 'Deadline', 'Notes', 'AboutC', 'Attach',\n",
      "       'Year', 'Month', 'IT'],\n",
      "      dtype='object')\n",
      "                                             jobpost          date  \\\n",
      "0  AMERIA Investment Consulting Company\\r\\nJOB TI...   Jan 5, 2004   \n",
      "1  International Research & Exchanges Board (IREX...   Jan 7, 2004   \n",
      "2  Caucasus Environmental NGO Network (CENN)\\r\\nJ...   Jan 7, 2004   \n",
      "3  Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...   Jan 7, 2004   \n",
      "4  Yerevan Brandy Company\\r\\nJOB TITLE:  Software...  Jan 10, 2004   \n",
      "\n",
      "                                               Title  \\\n",
      "0                            Chief Financial Officer   \n",
      "1  Full-time Community Connections Intern (paid i...   \n",
      "2                                Country Coordinator   \n",
      "3                                     BCC Specialist   \n",
      "4                                 Software Developer   \n",
      "\n",
      "                                           Company AnnouncementCode Term  \\\n",
      "0             AMERIA Investment Consulting Company              NaN  NaN   \n",
      "1  International Research & Exchanges Board (IREX)              NaN  NaN   \n",
      "2        Caucasus Environmental NGO Network (CENN)              NaN  NaN   \n",
      "3                                     Manoff Group              NaN  NaN   \n",
      "4                           Yerevan Brandy Company              NaN  NaN   \n",
      "\n",
      "  Eligibility Audience StartDate                               Duration  ...  \\\n",
      "0         NaN      NaN       NaN                                    NaN  ...   \n",
      "1         NaN      NaN       NaN                               3 months  ...   \n",
      "2         NaN      NaN       NaN  Renewable annual contract\\r\\nPOSITION  ...   \n",
      "3         NaN      NaN       NaN                                    NaN  ...   \n",
      "4         NaN      NaN       NaN                                    NaN  ...   \n",
      "\n",
      "  Salary                                       ApplicationP OpeningDate  \\\n",
      "0    NaN  To apply for this position, please submit a\\r\\...         NaN   \n",
      "1    NaN  Please submit a cover letter and resume to:\\r\\...         NaN   \n",
      "2    NaN  Please send resume or CV toursula.kazarian@......         NaN   \n",
      "3    NaN  Please send cover letter and resume to Amy\\r\\n...         NaN   \n",
      "4    NaN  Successful candidates should submit\\r\\n- CV; \\...         NaN   \n",
      "\n",
      "                                        Deadline Notes  \\\n",
      "0                                26 January 2004   NaN   \n",
      "1                                12 January 2004   NaN   \n",
      "2  20 January 2004\\r\\nSTART DATE:  February 2004   NaN   \n",
      "3      23 January 2004\\r\\nSTART DATE:  Immediate   NaN   \n",
      "4                         20 January 2004, 18:00   NaN   \n",
      "\n",
      "                                              AboutC Attach  Year Month     IT  \n",
      "0                                                NaN    NaN  2004     1  False  \n",
      "1  The International Research & Exchanges Board (...    NaN  2004     1  False  \n",
      "2  The Caucasus Environmental NGO Network is a\\r\\...    NaN  2004     1  False  \n",
      "3                                                NaN    NaN  2004     1  False  \n",
      "4                                                NaN    NaN  2004     1   True  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(jobs.shape)\n",
    "print(jobs.columns)\n",
    "print(jobs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfd0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', ' ', str(text))\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "resumes['cleaned_text'] = resumes['Resume'].apply(clean_text)\n",
    "jobs['Category'] = resumes['Category'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53ddc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobs[['Title', 'JobDescription', 'RequiredQual']]\n",
    "jobs = jobs.fillna(\"\")\n",
    "for col in ['Title', 'JobDescription', 'RequiredQual']:\n",
    "    jobs[col] = jobs[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab6ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Resume  \\\n",
      "0  skills programming languages python pandas num...   \n",
      "1  skills programming languages python pandas num...   \n",
      "2  education details may 2013 to may 2017 b e uit...   \n",
      "3  education details may 2013 to may 2017 b e uit...   \n",
      "4  areas of interest deep learning control system...   \n",
      "\n",
      "                                            Job_Desc  Label  \n",
      "0  software engineer will take part in design and...      1  \n",
      "1  under the overall guidance of the undp climate...      0  \n",
      "2  epam systems is actively looking for ms sql da...      1  \n",
      "3  veya investments limited is looking for a prof...      0  \n",
      "4  it department of chamber of commerce and indus...      1  \n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "pairs = []\n",
    "\n",
    "for idx, row in resumes.iterrows():\n",
    "    resume_text = row['cleaned_text']\n",
    "    category = row['Category']\n",
    "\n",
    "    # Positive job match\n",
    "    positive_jobs = jobs[jobs['Title'].str.contains(category.split()[0], case=False, na=False)]\n",
    "    if not positive_jobs.empty:\n",
    "        job_row = positive_jobs.sample(1).iloc[0]\n",
    "        pairs.append([resume_text, job_row['JobDescription'], 1])\n",
    "\n",
    "    # Negative job match\n",
    "    negative_jobs = jobs[~jobs['Title'].str.contains(category.split()[0], case=False, na=False)]\n",
    "    if not negative_jobs.empty:\n",
    "        job_row = negative_jobs.sample(1).iloc[0]\n",
    "        pairs.append([resume_text, job_row['JobDescription'], 0])\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=['Resume', 'Job_Desc', 'Label'])\n",
    "print(pairs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137a470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df.to_csv(\"D:/resume screener/my_datasets/training_pairs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3c514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3cf8fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gg185\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gg185\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea806bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec25155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Resume_clean  \\\n",
      "0  skill programming language python panda numpy ...   \n",
      "1  skill programming language python panda numpy ...   \n",
      "2  education detail may 2013 may 2017 b e uit rgp...   \n",
      "3  education detail may 2013 may 2017 b e uit rgp...   \n",
      "4  area interest deep learning control system des...   \n",
      "\n",
      "                                      Job_Desc_clean  Label  \n",
      "0  software engineer take part design implementat...      1  \n",
      "1  overall guidance undp climate change programme...      0  \n",
      "2  epam system actively looking m sql database de...      1  \n",
      "3  veya investment limited looking professional o...      0  \n",
      "4  department chamber commerce industry ra associ...      1  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    text = re.sub(r'/d+', '', text)\n",
    "    text = text.strip()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "pairs_df['Resume_clean'] = pairs_df['Resume'].apply(clean_text)\n",
    "pairs_df['Job_Desc_clean'] = pairs_df['Job_Desc'].apply(clean_text)\n",
    "\n",
    "print(pairs_df[['Resume_clean', 'Job_Desc_clean', 'Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f708a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb4e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "221539b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings = model_bert.encode(pairs_df['Resume_clean'].tolist(), convert_to_numpy=True)\n",
    "job_embeddings = model_bert.encode(pairs_df['Job_Desc_clean'].tolist(), convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f2003f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([resume_embeddings, job_embeddings])\n",
    "y = pairs_df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b67cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7079889807162535\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.74       196\n",
      "           1       0.71      0.62      0.66       167\n",
      "\n",
      "    accuracy                           0.71       363\n",
      "   macro avg       0.71      0.70      0.70       363\n",
      "weighted avg       0.71      0.71      0.71       363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f8a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ea4b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sbert_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"resume_match_model.pkl\")\n",
    "joblib.dump(model_bert, \"sbert_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6e796",
   "metadata": {},
   "source": [
    "Fine Tuning BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f76b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7866f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pd.read_csv(\"D:/resume screener/my_datasets/training_pairs.csv\")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    pairs_df[['Resume', 'Job_Desc']].values.tolist(),\n",
    "    pairs_df['Label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dce71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class ResumeJobDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        resume, job = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            resume,\n",
    "            job,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "413a87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ResumeJobDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = ResumeJobDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0afaa50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
